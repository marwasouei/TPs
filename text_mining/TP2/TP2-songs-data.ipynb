{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**References**\n",
    "\n",
    "\n",
    "https://www.datacamp.com/community/tutorials/stemming-lemmatization-python\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.cosine_similarity.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "songs_df = pd.read_csv ('./songs_train.csv', header=0, low_memory=False)\n",
    "songs_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "songs_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "songs_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the first row\n",
    "\n",
    "songs_df.iloc[1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data type of each variable (column)\n",
    "\n",
    "songs_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Explore categorical/text variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "songs_df.dtypes.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "songs_df.dtypes[songs_df.dtypes==object]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "songs_df[songs_df.dtypes[songs_df.dtypes==object].index].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_variables = ['artist_mbtags','terms','location','title']\n",
    "songs_df[text_variables].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "songs_df[text_variables].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of missing values in each column\n",
    "\n",
    "songs_df[text_variables].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering of `title` variable\n",
    "Our goal here is to extract the most important keywords from the `text` column and use them to represent the title as a feature vector instead of plain text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_corpus = songs_df.title.tolist()\n",
    "title_corpus[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0) Visual exploration of corpus\n",
    "\n",
    "Visual exploration will help us detect noise in the corpus (so that we clean it in the next step).\n",
    "\n",
    "We will be looking for two types of noise:\n",
    "\n",
    "- non-word characters; we will do this by visualizing the distribution of characters in corpus.\n",
    "- non-English words; we will do this via simple visual inspection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### QUESTIONS\n",
    "\n",
    "Execute the cells in this section then answer the following questions.\n",
    "\n",
    "1. Are there any **strange** characters in this corpus, i.e. that are **not** English word letters (a-z), punctuation, or numbers?\n",
    "2. If you answered yes to the above question, are any of these strange characters **very frequent** ? Justify your answer. *Hint*: you can use `fdist1` to determine the frequency of any character.\n",
    "(If yes, then you will need to make sure these characters are removed in the next step ...)\n",
    "3. Based on the distribution plot below, the top 20 characters cover what fraction of all character occurrences in this corpus?\n",
    "4. Based on simple visual inspection of the corpus, do you notice any non-English words? If yes, are they a few or alot?  (If yes, then we would need a way of removing them in the next step ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import FreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the corpus from list of strings to a single string (i.e. sequence of characters)\n",
    "\n",
    "corpus_char_list = \"\\n\".join(title_corpus)\n",
    "type(corpus_char_list),len(corpus_char_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of FreqDist class and then use it to count the number of occurrences of each character in the corpus\n",
    "\n",
    "fdist1 = FreqDist([c for c in corpus_char_list])\n",
    "type(fdist1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The FreqDist data type is in fact similar to a dictionary\n",
    "\n",
    "fdist1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total number of characters in this coprus:\",fdist1.N())\n",
    "\n",
    "print(\"Number of DISTINCT characters in this corpus:\",fdist1.B())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('List of distinct characters in the corpus, sorted by their Unicode values:\\n')\n",
    "print(sorted(list(fdist1.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can now obtain the number of occurrences of any character using the fdist1 object\n",
    "\n",
    "print(\"The number of occurrences of the character 'z':\", fdist1['z'])\n",
    "\n",
    "for x in ['a','b','c','/','[',';','-','?','!','~']:\n",
    "    print(\"The number of occurrences of the character '%c': %d\" % (x, fdist1[x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#?fdist1.most_common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The most_common() method sorts the characters in decreasing order of frequency\n",
    "\n",
    "print('The 10 most frequent characters in the corpus and their corresponding number of occurrences:')\n",
    "fdist1.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#?fdist1.plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The plot() method of the FreqDist class plots the distribution of the most frequent characters\n",
    "fig=fdist1.plot(20,cumulative=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Text cleaning\n",
    "\n",
    "We are now going to apply the following sequence of text cleaning operations to **every** document in the corpus. \n",
    "\n",
    "- a) remove non-word useless characters (if there are any)\n",
    "- b) convert to lowercase\n",
    "- c) tokenize (convert sequence of characters to sequence of words)\n",
    "- d) remove stop words\n",
    "- e) remove useless words (too short or too long words)\n",
    "- f) stemming\n",
    "\n",
    "We will do most of these tasks using functions from the ``NLTK`` library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a)b)c) Remove useless characters & convert to lowercase & tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTIONS**\n",
    "\n",
    "Execute the cells in this section then answer the following questions:\n",
    "\n",
    "1. What does the instance of RegexpTokenizer class do; which characters does it keep?\n",
    "2. Modify the value of `tokenization_regexp` so that it only keeps a-z characters then re-execute the cells in this section.\n",
    "3. What is the smallest and largest number of words in a song title?\n",
    "4. How many song titles contain the word 'love'?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an instance of the RegexpTokenizer class\n",
    "\n",
    "tokenization_regexp = '[^_\\W]+'\n",
    "tokenizer = RegexpTokenizer(tokenization_regexp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize an example document by calling the tokenizer() method of this class\n",
    "tokenizer.tokenize(\"It's still early - it is just 3 o'clock now!! :) (_) \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply tokenizer to each document (song title) in our corpus\n",
    "title_corpus_words = [tokenizer.tokenize(doc.lower()) for doc in title_corpus]\n",
    "type(title_corpus_words),len(title_corpus_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the first 5 documents in the tokenized corpus\n",
    "title_corpus_words[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now visualize distribution of number of words per song title. This is just to verify the results of tokenization ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list containing the number of words in each document\n",
    "df = pd.Series([len(doc) for doc in title_corpus_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cumulative frequency distribution of number of words in each song title\n",
    "\n",
    "df.plot.hist(title='song title length (in number of words)', cumulative=True)\n",
    "fig=plt.xlabel('number of words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display song titles that contain only one word\n",
    "\n",
    "L = [doc[0] for doc in title_corpus_words if len(doc)==1]\n",
    "print(len(L))\n",
    "print(L[0:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display documents that contain more than 20 words\n",
    "\n",
    "L = [' '.join(doc) for doc in title_corpus_words if len(doc)>=20]\n",
    "print(len(L))\n",
    "L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d) Remove stopwords\n",
    "We will use NLTK's default list of stop worsd for the English language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load list of stopwords from NLTK library\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You might need to download the set of stop words the first time\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load stop words\n",
    "stop_words_en = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(stop_words_en))\n",
    "print(len(stop_words_en))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the first 10 stop words\n",
    "stop_words_en[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stop words from our corpus\n",
    "title_corpus_words_2 = [[word for word in doc  if word not in stop_words_en] for doc in title_corpus_words]\n",
    "type(title_corpus_words_2),len(title_corpus_words_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### e) remove useless words based on word length\n",
    "\n",
    "- Very short words are usually not very meaningful.\n",
    "- Very long words might be either spelling mistakes, or elongated words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTIONS**\n",
    "\n",
    "1. remove all words that contain <= 2 characters or more than 12 characters from all titles in `title_corpus_words_2`. Put the result in a new list called `title_corpus_words_3`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create set of all distinct words in corpus\n",
    "distinct_words_set = {word for doc in title_corpus_words_2 for word in doc}\n",
    "type(distinct_words_set),len(distinct_words_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_len_df = pd.Series([len(word) for word in distinct_words_set], index=list(distinct_words_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics of word length\n",
    "words_len_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram of word length\n",
    "fig = words_len_df.plot.hist(title=\"word length\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = words_len_df.plot.hist(title=\"word length\", cumulative=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many words have length <= 2\n",
    "words_len_df[words_len_df<=2].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# which words have length <= 2\n",
    "print(sorted(words_len_df[words_len_df<=2].index.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many words contain more than 15 characters\n",
    "words_len_df[words_len_df>=15].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sorted(words_len_df[words_len_df>=15].index.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove words that contain <= 2 characters or >= 15\n",
    "\n",
    "title_corpus_words_3 = ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify type and length\n",
    "\n",
    "assert type(title_corpus_words_3)==list and len(title_corpus_words_3)==len(title_corpus_words_2) and type(title_corpus_words_3[0])==list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### f) Stemming\n",
    "- All stemming methods are heuristic; there is no perfect stemming method; they all make mistakes.\n",
    "- We will try the famous Porter method from the `NLTK` library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### QUESTIONS\n",
    "\n",
    "Execute the cells below then answer the following questions:\n",
    "\n",
    "1. This stemming method reduces/shrinks the vocabulary by how much (fraction)?\n",
    "2. Give 3 example errors of this stemming method; where an error is when two unrelated words are mapped to the same stem word.\n",
    "3. Do you think there are too many stemming erros and we should just NOT use stemming? Explain.\n",
    "4. Can you suggest a simple way (based on simple string operations) to reduce the amount of errors of this stemming method?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create instance of class\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_corpus_words_4 = [[stemmer.stem(word) for word in doc] for doc in title_corpus_words_3]\n",
    "type(title_corpus_words_4),len(title_corpus_words_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_corpus_words_4[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyze the results of stemming method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of distinct words BEFORE stemming\n",
    "len({word for doc in title_corpus_words_3 for word in doc})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of distinct words AFTER stemming\n",
    "len({word for doc in title_corpus_words_4 for word in doc})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionnary containing each word and its corresponding stemmed word\n",
    "distinct_words = {word for doc in title_corpus_words_3 for word in doc} # set of distinct words in corpus BEFORE stemming\n",
    "from collections import defaultdict\n",
    "d1 = defaultdict(list)\n",
    "for w in distinct_words:\n",
    "    d1[stemmer.stem(w)].append(w)\n",
    "len(d1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# key = stemmed word, value = list of all words mapped to this stemmed word\n",
    "d1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display words which are mapped to the SAME word\n",
    "for k,v in d1.items():\n",
    "    if len(v)>1:\n",
    "        print(k,v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare clean corpus for BOW in Scikit-learn\n",
    "The BOW module in ``Scikit-Learn`` library requires the input documents as a list of strings, and not as a list of words. Therefore we are going to concatenate the words in our cleaned tokenized corpus .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate the words in the cleaned corpus\n",
    "title_corpus_clean = [' '.join(doc) for doc in title_corpus_words_4]\n",
    "len(title_corpus_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_corpus[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_corpus_clean[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Vector representation using boolean BOW\n",
    "\n",
    "**QUESTIONS**\n",
    "\n",
    "1. Create an instance of the `CounVectorizer` class, with min_df=3 and max_df=0.9. Put this instance in a variable called `title_vec`. Make sure it corresponds to **boolean** bag-of-words.\n",
    "2. Fit this instance on the text documents in `title_corpus_clean`. How many words are there in the vocabulary?\n",
    "3. Create the document-term matrix for `title_corpus_clean`, and put the result in a variable called `title_dtm`. What is the size of this matrix? \n",
    "4. What are the minimum and maximum values of this matrix?  Does this make sense?\n",
    "5. How many rows of this matrix contain all zeros? How many rows contain only one non-zero value?  Hint: call the sum() method with argument axis=1, then convert the result to an array ...\n",
    "6. Calculate the cosine similarity between all pairs of rows in `title_dtm` by calling the `cosine_similarity` function from the `sklearn.metrics.pairwise` module (this function has already been imported for you above).\n",
    "7. Print all pairs of song titles that have cosine similarity above 0.9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional Bonus Questions\n",
    "\n",
    "Answer the question that seems easier to you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "\n",
    "1. Cluster the song titles using the feature vectors in `title_dtm`. You are free to use any clustering method (for example kmeans in sklearn ...). Also, you will need to select an appropriate number of clusters.\n",
    "2. Print the song titles in the largest cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "\n",
    "Copy this file into a new .ipynb file and then repeat all the work for the `terms` column instead of the `title` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
