{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP1\n",
    "References:\n",
    "\n",
    "https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html\n",
    "\n",
    "https://github.com/makcedward/nlp/blob/master/sample/nlp-bag_of_words.ipynb\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6641, 2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "with open(r'C:\\Users\\Lenovo\\Documents\\cours_dataminig\\newsgroups_data\\newsgroups_data.pickle', 'rb') as f:\n",
    "    newsgroups_data_df = pickle.load(f)\n",
    "newsgroups_data_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message</th>\n",
       "      <th>group_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hello folks, I've a super scope 6 for sale, it...</td>\n",
       "      <td>misc.forsale</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I have for sale the following:\\n\\n\\tHewlett Pa...</td>\n",
       "      <td>misc.forsale</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\nThis is definitely the wrong newsgroup for t...</td>\n",
       "      <td>sci.electronics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Acorn Software, Inc. has 3 tape drives (curren...</td>\n",
       "      <td>misc.forsale</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\\n[KAAN] Who the hell is this guy David Davidi...</td>\n",
       "      <td>talk.politics.mideast</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             message            group_label\n",
       "0  Hello folks, I've a super scope 6 for sale, it...           misc.forsale\n",
       "1  I have for sale the following:\\n\\n\\tHewlett Pa...           misc.forsale\n",
       "2  \\nThis is definitely the wrong newsgroup for t...        sci.electronics\n",
       "3  Acorn Software, Inc. has 3 tape drives (curren...           misc.forsale\n",
       "4  \\n[KAAN] Who the hell is this guy David Davidi...  talk.politics.mideast"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsgroups_data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sci.med                  990\n",
       "rec.autos                990\n",
       "sci.space                987\n",
       "sci.electronics          984\n",
       "misc.forsale             975\n",
       "talk.politics.mideast    940\n",
       "talk.politics.misc       775\n",
       "Name: group_label, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsgroups_data_df.group_label.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "******"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector representation of documents using simple BOW\n",
    "\n",
    "We will do this using the CountVectorizer class from the `sklearn` library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions - part 1**\n",
    "\n",
    "Execute the cells below, then answer the following questions based on the output of these cells. You do not need to write new code.\n",
    "\n",
    "1. What is the size of the document-term matrix `dtm1`, and what is the fraction of non-zero values in it?\n",
    "2. What is the maximum value in `dtm1`, and what does this value mean?\n",
    "3. How many words does the vocabulary contain?\n",
    "4. Give the words that correspond to the first 10 columns of `dtm1`.\n",
    "5. Are the words in `vocab1` ordered arbitrarily or in some specific order?\n",
    "6. Read the documentation of the `CountVectorizer` class using the command `?CountVectorizer`. Without writing any code, what will happen if we modify the parameter `binary = True` ?\n",
    "7. Repeat the previous question by modifying the parameter `lowercase = False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read documentation of the CountVectorizer class's constructor.\n",
    "# Notice the default values of its parameters.\n",
    "?CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Set values of control parameters of the Vectorizer classes that we will use\n",
    "\n",
    "max_features = 5000\n",
    "max_df = 0.9\n",
    "min_df = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=0.9, max_features=None, min_df=3,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words='english',\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create an instance of the class with the desired parameter values.\n",
    "vect1 = CountVectorizer(max_df=max_df, min_df=min_df, stop_words='english', binary = False)\n",
    "\n",
    "# create vocabulary based on given corpus by calling the fit() method.\n",
    "vect1.fit(newsgroups_data_df.message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6641, 18939)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create the document-term matrix (DTM) of a given corpus by calling the transform() method\n",
    "dtm1 = vect1.transform(newsgroups_data_df.message)\n",
    "dtm1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse.csr.csr_matrix"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# what is the data type of this matrix?\n",
    "#  dtm is stored in a special data structure (class) called 'scipy.sparse.csr.csr_matrix'\n",
    "type(dtm1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "406455"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# what is the number of non-zero values in this matrix?\n",
    "dtm1.nnz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 185)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the minimum and maximum values\n",
    "dtm1.min(),dtm1.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## let's store the vocabulary in a list for convenience\n",
    "# the first element of vocab corresponds to the first column of dtm\n",
    "# the second element of vocab corresponds to the first column of dtm\n",
    "# etc.\n",
    "vocab1 = vect1.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "18939\n"
     ]
    }
   ],
   "source": [
    "print(type(vocab1))\n",
    "print(len(vocab1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['00', '000', '0000', '00000', '000k', '001', '00pm', '01', '011', '015', '01730', '02', '02106', '02115', '02173', '03', '030', '0300', '04', '05', '0500', '06', '0663', '0674', '07', '08', '08003', '081', '09', '095', '0l', '10', '100', '1000', '1000w', '100hz', '100k', '100mhz', '100ns', '101', '102', '1024x768', '103', '1031', '104', '1047', '105', '106', '107', '1070']\n"
     ]
    }
   ],
   "source": [
    "# The first 50 words of the vocabulary\n",
    "print(vocab1[0:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['yugo', 'yugoslav', 'yugoslavia', 'yunusova', 'yup', 'yuppie', 'yuppies', 'yuri', 'yusuf', 'yxy4145', 'yyz', 'z28', 'z80', 'za', 'zabitlari', 'zahal', 'zangezour', 'zangibasar', 'zaphod', 'zbib', 'zeal', 'zealand', 'zen', 'zener', 'zenith', 'zeos', 'zero', 'zeus', 'zhiguli', 'zilfi', 'zilkade', 'zillion', 'zinc', 'zion', 'zionism', 'zionist', 'zionists', 'zip', 'zivin', 'zodiacal', 'zoloft', 'zombies', 'zone', 'zones', 'zoo', 'zoom', 'zuma', 'zur', 'zwischen', 'zx']\n"
     ]
    }
   ],
   "source": [
    "# The last 50 words of the vocabulary\n",
    "print(vocab1[-50:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Your answers\n",
    "\n",
    "#Q1.\n",
    "the shape of dtm1 are 6641 documents, 18939terms. there are 406455/(6641*18939)= 0.00323"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Q2.\n",
    "the maximum value is 185.It means the maximum frequency of a world in a document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Q3.\n",
    "Number of words does the vocabulary contains is 18939"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['00', '000', '0000', '00000', '000k', '001', '00pm', '01', '011', '015']\n"
     ]
    }
   ],
   "source": [
    "#Q4. The first 10 words of the vocabulary:\n",
    "print(vocab1[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Q5. The words are ordered by alphabet as we noticed in the cell 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Q6.Binary=True it means we will activate the first version of the bag-of-words (the value are 0 or 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Q7. Lowercase=False we will have uppercase words and lowercase words "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions - part 2**\n",
    "\n",
    "In each of the following questions, you need to re-build the vocabulary and the DTM matrix. You can copy the code above and modify it as desired.\n",
    "\n",
    "1. Change the `token_pattern` parameter of the original call to `CountVectorizer()` so that only words that contain the letters 'a' to 'z' and the '-' character are accepted in the vocabulary. How many words does the new vocabulary contain and what are the first 10 words of this vocabulary?\n",
    "\n",
    "2. Add the following parameter to the original call to `CountVectorizer()`: ngram_range=(1,2). Determine the number of words of the new vocabulary, and give 10 words in this vocabulary that are not in `vocab1`.\n",
    "\n",
    "3. Change the parameters of the original call to `CountVectorizer()` using the new values below, but modify each parameter separately (not simulateneously). For each modification, explain how and why the vocabulary and DTM have changed compared to `vocab1` and `dtm1`.\n",
    "\n",
    "       stop_words = None\n",
    "       max_df = 0.7\n",
    "       min_df = 1\n",
    "       max_features = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Q1\n",
    "vect2 = CountVectorizer(max_df=max_df, min_df=min_df, stop_words='english', binary = False,token_pattern='[a-zA-Z]+-?[a-zA-Z]+')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=0.9, max_features=None, min_df=3,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words='english',\n",
       "        strip_accents=None, token_pattern='[a-zA-Z]+-?[a-zA-Z]+',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "vect2.fit(newsgroups_data_df.message)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm2 = vect2.transform(newsgroups_data_df.message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6641, 17978)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtm2.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "382235"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# what is the number of non-zero values in this matrix?\n",
    "dtm2.nnz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 153)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the minimum and maximum values\n",
    "dtm2.min(),dtm2.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vocab2 = vect2.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "17978\n"
     ]
    }
   ],
   "source": [
    "print(type(vocab2))\n",
    "print(len(vocab2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aa', 'aaa', 'aamir', 'aaron', 'aas', 'ab', 'abandon', 'abandoned', 'abandoning', 'abbey']\n"
     ]
    }
   ],
   "source": [
    "# The first 50 words of the vocabulary\n",
    "print(vocab2[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Q1. 17978 words in the vocab2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q2.\n",
    "vect3 = CountVectorizer(max_df=max_df, min_df=min_df, stop_words='english', binary = False,ngram_range=(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=0.9, max_features=None, min_df=3,\n",
       "        ngram_range=(1, 2), preprocessor=None, stop_words='english',\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "vect3.fit(newsgroups_data_df.message)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm3 = vect3.transform(newsgroups_data_df.message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6641, 42246)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtm3.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "517520"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# what is the number of non-zero values in this matrix?\n",
    "dtm3.nnz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 185)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the minimum and maximum values\n",
    "dtm3.min(),dtm3.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vocab3 = vect3.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "42246\n"
     ]
    }
   ],
   "source": [
    "print(type(vocab3))\n",
    "print(len(vocab3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of non common words are 23307 example of 10 words ['bari unknown', 'harbord anadolu', 'second round', 'know secretive', 'having difficulty', 'real estate', 'woke dro', 'teblig olunacak', 'ken mitchell', 'rider old']\n"
     ]
    }
   ],
   "source": [
    "idx=set(vocab3)-set(vocab1)\n",
    "print('the number of non common words are',len(idx),\"example of 10 words\",list(idx)[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NNZ 439339\n",
      "size (6641, 22288)\n",
      "min 0 max 185\n",
      "type <class 'list'>\n",
      "length 22288\n"
     ]
    }
   ],
   "source": [
    "# lowercase = False\n",
    "vect4 = CountVectorizer(max_df=max_df, min_df=min_df, stop_words='english', binary = False,lowercase = False)\n",
    "vect4.fit(newsgroups_data_df.message)\n",
    "dtm4 = vect4.transform(newsgroups_data_df.message)\n",
    "vocab4 = vect4.get_feature_names()\n",
    "print('NNZ',dtm4.nnz)\n",
    "print('size',dtm4.shape)\n",
    "print('min',dtm4.min(),'max',dtm4.max())\n",
    "print('type',type(vocab4))\n",
    "print('length',len(vocab4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "puisque lowercase=False le nombre de mot a augmenté de (22288-18939)=3349 mots, nous etudions les mots majuscules et minuscules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NNZ 406455\n",
      "size (6641, 18939)\n",
      "min 0 max 185\n",
      "type <class 'list'>\n",
      "length 18939\n"
     ]
    }
   ],
   "source": [
    "#Q3.\n",
    "#max_df = 0.7\n",
    "max_df1 = 0.7\n",
    "vect5 = CountVectorizer(max_df=max_df1, min_df=min_df, stop_words='english', binary = False)\n",
    "vect5.fit(newsgroups_data_df.message)\n",
    "dtm5 = vect5.transform(newsgroups_data_df.message)\n",
    "vocab5 = vect5.get_feature_names()\n",
    "print('NNZ',dtm5.nnz)\n",
    "print('size',dtm5.shape)\n",
    "print('min',dtm5.min(),'max',dtm5.max())\n",
    "print('type',type(vocab5))\n",
    "print('length',len(vocab5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of non common words are 0 example of 10 words []\n"
     ]
    }
   ],
   "source": [
    "idx=set(vocab5)-set(vocab1)\n",
    "print('the number of non common words are',len(idx),\"example of 10 words\",list(idx)[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pas de changement car diminuer le max_df par 0.3 n'a pas d'effet sur cette dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NNZ 446163\n",
      "size (6641, 51354)\n",
      "min 0 max 185\n",
      "type <class 'list'>\n",
      "length 51354\n"
     ]
    }
   ],
   "source": [
    "#min_df\n",
    "min_df1 = 1\n",
    "vect6 = CountVectorizer(max_df=max_df, min_df=min_df1, stop_words='english', binary = False)\n",
    "vect6.fit(newsgroups_data_df.message)\n",
    "dtm6 = vect6.transform(newsgroups_data_df.message)\n",
    "vocab6 = vect6.get_feature_names()\n",
    "print('NNZ',dtm6.nnz)\n",
    "print('size',dtm6.shape)\n",
    "print('min',dtm6.min(),'max',dtm6.max())\n",
    "print('type',type(vocab6))\n",
    "print('length',len(vocab6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "changement dans le nombre de mot.Il a augmenté de 51354-18939=32415 puisque nous avons diminué le min_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NNZ 237615\n",
      "size (6641, 2000)\n",
      "min 0 max 185\n",
      "type <class 'list'>\n",
      "length 2000\n"
     ]
    }
   ],
   "source": [
    "#max_features = 2000\n",
    "vect7 = CountVectorizer(max_df=max_df, min_df=min_df, stop_words='english', binary = False, max_features = 2000)\n",
    "vect7.fit(newsgroups_data_df.message)\n",
    "dtm7= vect7.transform(newsgroups_data_df.message)\n",
    "vocab7 = vect7.get_feature_names()\n",
    "print('NNZ',dtm7.nnz)\n",
    "print('size',dtm7.shape)\n",
    "print('min',dtm7.min(),'max',dtm7.max())\n",
    "print('type',type(vocab7))\n",
    "print('length',len(vocab7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nombre de mots a diminué de 18939-2000=16939 puisue nous avons limité le nombre de max_feature alors que dans vect1 il est par défaut "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NNZ 611258\n",
      "size (6641, 19242)\n",
      "min 0 max 716\n",
      "type <class 'list'>\n",
      "length 19242\n"
     ]
    }
   ],
   "source": [
    "#stop_words=None\n",
    "vect8 = CountVectorizer(max_df=max_df, min_df=min_df, stop_words=None, binary = False)\n",
    "vect8.fit(newsgroups_data_df.message)\n",
    "dtm8= vect8.transform(newsgroups_data_df.message)\n",
    "vocab8 = vect8.get_feature_names()\n",
    "print('NNZ',dtm8.nnz)\n",
    "print('size',dtm8.shape)\n",
    "print('min',dtm8.min(),'max',dtm8.max())\n",
    "print('type',type(vocab8))\n",
    "print('length',len(vocab8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "maximum of frequency in the the document increases it becomes 716 and the number of words also it becomes 19242 as we took into consideration the stop_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector representation of documents using tfidf BOW\n",
    "\n",
    "We will do this using the TfidfVectorizer class from the `sklearn` library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions**\n",
    "\n",
    "1. Read the documentation of the`TfidfVectorizer` class.\n",
    "2. Call the constructor of this class using the following parameter values: `max_df=max_df, min_df=min_df, max_features=max_features, stop_words='english'`. Put the result in a variable called `vect2`.\n",
    "3. Call the `fit_transform` in order to create the vocabulary and the DTM matrix in one step using the text documents in `newsgroups_data_df.message`. Put the result in a variable called `dtm2`.\n",
    "4. Determine the minimum and maximum values of `dtm2`.\n",
    "5. Is the vocabulary of vect2 different than `vocab1`? You can use set operations for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read documentation of the TfidfVectorizer class\n",
    "?TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q2.\n",
    "vect2=TfidfVectorizer(max_df=max_df, min_df=min_df, max_features=max_features, stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q3.\n",
    "vect2.fit_transform(newsgroups_data_df.message)\n",
    "dtm2= vect2.transform(newsgroups_data_df.message)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min 0.0 max 1.0\n"
     ]
    }
   ],
   "source": [
    "#Q4.\n",
    "print('min',dtm2.min(),'max',dtm2.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NNZ 317215\n",
      "size (6641, 5000)\n",
      "type <class 'list'>\n",
      "length 5000\n"
     ]
    }
   ],
   "source": [
    "vocab2 = vect2.get_feature_names()\n",
    "\n",
    "print('NNZ',dtm2.nnz)\n",
    "print('size',dtm2.shape)\n",
    "\n",
    "print('type',type(vocab2))\n",
    "print('length',len(vocab2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of common words are 5000\n"
     ]
    }
   ],
   "source": [
    "idx=set(vocab2) & set(vocab1)\n",
    "print('the number of common words are',len(idx))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "le nombre de mot en commun est tout les element de vocab2 \n",
    "\n",
    "Le nombre de mots a diminué 18939-5000=13939 and the NNZ becomes 317215"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "******"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions**\n",
    "\n",
    "1. Copy `dtm1` into a new variable called `X` and copy `df.group_label` into a new variable called `y`\n",
    "2. split `X` and `y` into train and test sets with test set size of 30%.\n",
    "3. Build a logistic regression model using the training set and calculate its accuracy on the test set\n",
    "4. Repeat the first 3 questions with `dtm2` instead of `dtm1`. Is the accuracy better or worse than with `dtm1`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q1.\n",
    "X=dtm1.copy()\n",
    "y=newsgroups_data_df.group_label.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q2.\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train1, X_test1, y_train1, y_test1 = train_test_split(X,y, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Lenovo\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.806823883592574\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "clf=LogisticRegression()\n",
    "clf.fit(X_train1, y_train1)\n",
    "test_predictions = clf.predict(X_test1)\n",
    "acc = accuracy_score(y_test1, test_predictions)\n",
    "print(\"accuracy\",acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1=dtm2.copy()\n",
    "y1=newsgroups_data_df.group_label.copy()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X1,y1, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.8183642749623683\n"
     ]
    }
   ],
   "source": [
    "clf=LogisticRegression()\n",
    "clf.fit(X_train, y_train)\n",
    "test_pred = clf.predict(X_test)\n",
    "acc = accuracy_score(y_test, test_pred)\n",
    "print(\"accuracy\",acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the accuracy of dtm2 is better than dtm1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
